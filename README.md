# Clustering
##clustering is one important task of unsupervised learning
###This project are the homework of my machine learning course
--------------------------------------------------------------
一、实验目的
实现k-means并在UCI数据集上测试。
二、实验原理
1.k-means原理简介
聚类是一种无监督学习方法，它将相似的对象归到同一个簇中。聚类的结果实际上基于一个假设前提:对象数据具有类内相似性尽量大，类间相似性尽量小。K-means聚类算法就是一种经典的基于划分的聚类方法。k-means是无监督学习方法的一种，但是仍然需要提供先验知识即数据最后划分簇的数目k，k是人工根据现实经验等条件选定的。 k-means算法的处理过程如下：首先，随机地选择k个对象，每个对象初始地代表了一个簇的中心;对剩余的每个对象，根据其与各簇中心的距离，将它赋给最近的簇;然后重新计算每个簇的平均值，作为簇的平均中心。 这个过程不断重复，直到准则函数收敛。
通常，准则函数采用平方误差准则，其定义如下：，这里E是数据集中所有对象的平方误差的总和，p是空间中的点，是簇的平均值。该目标函数使生成的簇尽可能紧凑独立，使用的距离度量是欧几里得距离,也可以用其他距离度量方式。
本实验便采用k-means聚类方法对样本数据对象进行聚类。该方法易实现，对不存在极大值的数据有很好的聚类效果，并且对大数据集有很好的伸缩性。该方法的缺点是可能会收敛到局部最小值，在大规模的数据集上收敛比较慢，且该聚类算法适用的数据集为数值型数据，所以标称型数据需要做相应的数值映射以方便距离的度量。
2.K-means算法实现流程
输入样本集，指定聚类的簇数目为，最大迭代次数为。
1）将簇划分初始化为，并随机选择k个样本作为初值的簇中心向量；
2）对于所有的样本和中心进行以下流程
a.采用L2范数即欧几里得距离法则计算每个样本和各个簇中心的距离，根据距离重新分配点所属的簇；
b.根据对应簇中的所有样本点计算新的簇中心；
c.如果所有的个簇中心没有发生变化或者迭代次数达到了最大值，则转到步骤3），否则迭代次数加1并转到步骤2）
3）输出簇划分
3.聚类算法评价指标
聚类算法的评价指标即对聚类效果的性能度量，常用的聚类性能度量指标F分为两种，一类是将聚类结果与某个“参考模型”相比较，称为“外部指标”；另一类是直接考察聚类结果而不利用任何参考模型，称为“外部指标”，比较常用的外部指标有：Jaccard系数，FM系数，Rand指数，常用的内部指标有DB指数，Dunn指数。这些指数都反映了聚类的目的，即簇内相似度越高越好，簇间相似度越低越好。本实验采用的是外部指标中Rand指数的变种ARI指数。    
三、实验步骤
实验中应用k-means算法进行UCI数据分析具体流程：
1）从UCI数据集网站下载鸢尾花数据文件Iris.data，将该文件转为csv格式；
2）读入Iris.csv文件，获取鸢尾花的种类标签和对应的数据特征；
3）初始化k-means算法中的簇的类别数目为k=3，并同时给定最大的迭代次 
 数；
4）运行k-means算法直到满足算法停止条件，此时准则函数收敛；
5）结果可视化采用打印鸢尾花数据4个特征维度中的3个特征维度，通过该结果简
单的观察一下聚类效果，然后采用ARI指数作为聚类效果的评价指标；
6）测试数据直接采用距离度量计算测试数据和收敛的k个簇中心的距离，
将测试点分配给距离最近的簇中心即可。

####如何运行程序
文件中包含三个文件：
iris.csv  （这个是UCI中经典的鸢尾花数据数据集）
run.py  （运行程序文件）
kmeans.py (kmeans类的实现文件)

在python终端或者IDE直接运行一下命令可以看到结果：
python run.py
